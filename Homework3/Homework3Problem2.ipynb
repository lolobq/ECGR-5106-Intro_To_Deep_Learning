{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQh5DaHjwY9v"
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "EVoW7-9jwUmo"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yoTtdtx6O6o"
   },
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wLgd056e6Q8B"
   },
   "source": [
    "Build the model for.LSTM and rnn.GRU for the tiny Shakespeare dataset, the data loader code is already provided.\n",
    "\n",
    "Train the models for the sequence of 20 and 30, report and compare training loss, validation accuracy, execution time for training, and computational and mode size complexities across the two models.\n",
    "Adjust the hyperparameters (fully connected network, number of hidden layers, and the number of hidden states) and compare your results (training and validation loss, computation complexity, model size, training and inference time, and the output sequence). Analyze their influence on accuracy, running time, and computational perplexity.\n",
    "What if we increase the sequence length to 50? Perform the training and report the accuracy and model complexity results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OvOb5EE27nZ1"
   },
   "source": [
    "LSTM for Sequences of 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "sAL7_-8A6TWB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Training Loss: 1.7044667768635697\n",
      "Epoch 2/20, Training Loss: 1.4805794546676654\n",
      "Epoch 3/20, Training Loss: 1.4196120505694063\n",
      "Epoch 4/20, Training Loss: 1.383898568175609\n",
      "Epoch 5/20, Training Loss: 1.3588185905726287\n",
      "Epoch 6/20, Training Loss: 1.3396095018139258\n",
      "Epoch 7/20, Training Loss: 1.32466255495142\n",
      "Epoch 8/20, Training Loss: 1.3111787827464265\n",
      "Epoch 9/20, Training Loss: 1.300564376636271\n",
      "Epoch 10/20, Training Loss: 1.2909448598214968\n",
      "Epoch 11/20, Training Loss: 1.2828321612161668\n",
      "Epoch 12/20, Training Loss: 1.275565198036493\n",
      "Epoch 13/20, Training Loss: 1.2688569088503334\n",
      "Epoch 14/20, Training Loss: 1.2631857866999479\n",
      "Epoch 15/20, Training Loss: 1.257508506027023\n",
      "Epoch 16/20, Training Loss: 1.2534969425105666\n",
      "Epoch 17/20, Training Loss: 1.2484862255899645\n",
      "Epoch 18/20, Training Loss: 1.2452146334769187\n",
      "Epoch 19/20, Training Loss: 1.241536570045022\n",
      "Epoch 20/20, Training Loss: 1.2385825407843583\n",
      "Total execution time for training: 368.2469711303711 seconds\n",
      "Accuracy on test set: 58.20195001681049%\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Download and prepare the dataset\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "response = requests.get(url)\n",
    "text = response.text\n",
    "\n",
    "sequence_length = 20\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_int = {ch: i for i, ch in enumerate(chars)}\n",
    "int_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "encoded_text = [char_to_int[ch] for ch in text]\n",
    "\n",
    "sequences = []\n",
    "targets = []\n",
    "for i in range(0, len(encoded_text) - sequence_length):\n",
    "    seq = encoded_text[i:i+sequence_length]\n",
    "    target = encoded_text[i+sequence_length]\n",
    "    sequences.append(seq)\n",
    "    targets.append(target)\n",
    "\n",
    "sequences = torch.tensor(sequences, dtype=torch.long)\n",
    "targets = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, sequences, targets):\n",
    "        self.sequences = sequences\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.sequences[index], self.targets[index]\n",
    "\n",
    "dataset = CharDataset(sequences, targets)\n",
    "\n",
    "batch_size = 128\n",
    "train_size = int(len(dataset) * 0.8)\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(dataset=test_dataset, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "# Step 2: Define the LSTM model\n",
    "class CharLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(CharLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        embedded = self.embedding(x)\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        output = self.fc(output[:, -1, :])\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(1, batch_size, self.hidden_size, device=device),\n",
    "                torch.zeros(1, batch_size, self.hidden_size, device=device))\n",
    "\n",
    "# Step 3: Instantiate the model, loss function, and optimizer\n",
    "input_size = len(chars)\n",
    "hidden_size = 256\n",
    "output_size = len(chars)\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = CharLSTM(input_size, hidden_size, output_size).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Step 4: Training the model\n",
    "epochs = 20\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        hidden = model.init_hidden(data.size(0))\n",
    "        output, hidden = model(data, hidden)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Total execution time for training: {execution_time} seconds\")\n",
    "\n",
    "# Step 5: Evaluate the model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        hidden = model.init_hidden(data.size(0))\n",
    "        output, hidden = model(data, hidden)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "print(f\"Accuracy on test set: {100 * correct / total}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZsSUe0qg7spH"
   },
   "source": [
    "LSTM for Sequences of 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "UL_6IEvG7vC5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Training Loss: 1.6950380157719929\n",
      "Epoch 2/20, Training Loss: 1.4720642535903983\n",
      "Epoch 3/20, Training Loss: 1.411861192732109\n",
      "Epoch 4/20, Training Loss: 1.3755707422242518\n",
      "Epoch 5/20, Training Loss: 1.3514177188644858\n",
      "Epoch 6/20, Training Loss: 1.3314216484326915\n",
      "Epoch 7/20, Training Loss: 1.316760920000979\n",
      "Epoch 8/20, Training Loss: 1.303829388860752\n",
      "Epoch 9/20, Training Loss: 1.2932665293509429\n",
      "Epoch 10/20, Training Loss: 1.2840459199212984\n",
      "Epoch 11/20, Training Loss: 1.27549685058208\n",
      "Epoch 12/20, Training Loss: 1.2689115162714417\n",
      "Epoch 13/20, Training Loss: 1.2621861946049326\n",
      "Epoch 14/20, Training Loss: 1.2566690268919376\n",
      "Epoch 15/20, Training Loss: 1.251834972042909\n",
      "Epoch 16/20, Training Loss: 1.2464262265466235\n",
      "Epoch 17/20, Training Loss: 1.242239686166221\n",
      "Epoch 18/20, Training Loss: 1.2380418947080873\n",
      "Epoch 19/20, Training Loss: 1.2343832001611548\n",
      "Epoch 20/20, Training Loss: 1.2304845427772606\n",
      "Total execution time for training: 486.49959897994995 seconds\n",
      "Accuracy on test set: 58.45620043662837%\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Download and prepare the dataset\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "response = requests.get(url)\n",
    "text = response.text\n",
    "\n",
    "sequence_length = 30\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_int = {ch: i for i, ch in enumerate(chars)}\n",
    "int_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "encoded_text = [char_to_int[ch] for ch in text]\n",
    "\n",
    "sequences = []\n",
    "targets = []\n",
    "for i in range(0, len(encoded_text) - sequence_length):\n",
    "    seq = encoded_text[i:i+sequence_length]\n",
    "    target = encoded_text[i+sequence_length]\n",
    "    sequences.append(seq)\n",
    "    targets.append(target)\n",
    "\n",
    "sequences = torch.tensor(sequences, dtype=torch.long)\n",
    "targets = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, sequences, targets):\n",
    "        self.sequences = sequences\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.sequences[index], self.targets[index]\n",
    "\n",
    "dataset = CharDataset(sequences, targets)\n",
    "\n",
    "batch_size = 128\n",
    "train_size = int(len(dataset) * 0.8)\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(dataset=test_dataset, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "# Step 2: Define the LSTM model\n",
    "class CharLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(CharLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        embedded = self.embedding(x)\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        output = self.fc(output[:, -1, :])\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(1, batch_size, self.hidden_size, device=device),\n",
    "                torch.zeros(1, batch_size, self.hidden_size, device=device))\n",
    "\n",
    "# Step 3: Instantiate the model, loss function, and optimizer\n",
    "input_size = len(chars)\n",
    "hidden_size = 256\n",
    "output_size = len(chars)\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = CharLSTM(input_size, hidden_size, output_size).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Step 4: Training the model\n",
    "epochs = 20\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        hidden = model.init_hidden(data.size(0))\n",
    "        output, hidden = model(data, hidden)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Total execution time for training: {execution_time} seconds\")\n",
    "\n",
    "# Step 5: Evaluate the model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        hidden = model.init_hidden(data.size(0))\n",
    "        output, hidden = model(data, hidden)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "print(f\"Accuracy on test set: {100 * correct / total}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gOVRJIOs8K-3"
   },
   "source": [
    "LSTM for Sequences of 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "gpN5GhcJ8NIa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Training Loss: 1.704880822818123\n",
      "Epoch 2/20, Training Loss: 1.4727437397343\n",
      "Epoch 3/20, Training Loss: 1.409704635969588\n",
      "Epoch 4/20, Training Loss: 1.3736175929024237\n",
      "Epoch 5/20, Training Loss: 1.3474052431011898\n",
      "Epoch 6/20, Training Loss: 1.3275314313475122\n",
      "Epoch 7/20, Training Loss: 1.3117519244797482\n",
      "Epoch 8/20, Training Loss: 1.2983366263389862\n",
      "Epoch 9/20, Training Loss: 1.286451895949044\n",
      "Epoch 10/20, Training Loss: 1.2770698783541663\n",
      "Epoch 11/20, Training Loss: 1.2686877055466388\n",
      "Epoch 12/20, Training Loss: 1.2611189105900267\n",
      "Epoch 13/20, Training Loss: 1.2536283979677878\n",
      "Epoch 14/20, Training Loss: 1.2471858707293089\n",
      "Epoch 15/20, Training Loss: 1.2413178995783916\n",
      "Epoch 16/20, Training Loss: 1.2363641978746642\n",
      "Epoch 17/20, Training Loss: 1.2315996338332371\n",
      "Epoch 18/20, Training Loss: 1.2276254321953912\n",
      "Epoch 19/20, Training Loss: 1.223973639861974\n",
      "Epoch 20/20, Training Loss: 1.2205282230661754\n",
      "Total execution time for training: 806.8999671936035 seconds\n",
      "Accuracy on test set: 59.020303134904445%\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Download and prepare the dataset\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "response = requests.get(url)\n",
    "text = response.text\n",
    "\n",
    "sequence_length = 50\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_int = {ch: i for i, ch in enumerate(chars)}\n",
    "int_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "encoded_text = [char_to_int[ch] for ch in text]\n",
    "\n",
    "sequences = []\n",
    "targets = []\n",
    "for i in range(0, len(encoded_text) - sequence_length):\n",
    "    seq = encoded_text[i:i+sequence_length]\n",
    "    target = encoded_text[i+sequence_length]\n",
    "    sequences.append(seq)\n",
    "    targets.append(target)\n",
    "\n",
    "sequences = torch.tensor(sequences, dtype=torch.long)\n",
    "targets = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, sequences, targets):\n",
    "        self.sequences = sequences\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.sequences[index], self.targets[index]\n",
    "\n",
    "dataset = CharDataset(sequences, targets)\n",
    "\n",
    "batch_size = 128\n",
    "train_size = int(len(dataset) * 0.8)\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "# Step 2: Define the LSTM model\n",
    "class CharLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(CharLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        embedded = self.embedding(x)\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        output = self.fc(output[:, -1, :])\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(1, batch_size, self.hidden_size, device=device),\n",
    "                torch.zeros(1, batch_size, self.hidden_size, device=device))\n",
    "\n",
    "# Step 3: Instantiate the model, loss function, and optimizer\n",
    "input_size = len(chars)\n",
    "hidden_size = 256\n",
    "output_size = len(chars)\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = CharLSTM(input_size, hidden_size, output_size).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Step 4: Training the model\n",
    "epochs = 20\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        hidden = model.init_hidden(data.size(0))\n",
    "        output, hidden = model(data, hidden)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Total execution time for training: {execution_time} seconds\")\n",
    "\n",
    "# Step 5: Evaluate the model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        hidden = model.init_hidden(data.size(0))\n",
    "        output, hidden = model(data, hidden)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "print(f\"Accuracy on test set: {100 * correct / total}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p4lCabnl7yRC"
   },
   "source": [
    "GRU for Sequences of 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "b5H172S17-4z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Training Loss: 1.693705230181886\n",
      "Epoch 2/20, Training Loss: 1.5016383575045782\n",
      "Epoch 3/20, Training Loss: 1.4563810346595734\n",
      "Epoch 4/20, Training Loss: 1.4295546779910244\n",
      "Epoch 5/20, Training Loss: 1.4137790482309585\n",
      "Epoch 6/20, Training Loss: 1.402241639874136\n",
      "Epoch 7/20, Training Loss: 1.392108776423347\n",
      "Epoch 8/20, Training Loss: 1.387681982150313\n",
      "Epoch 9/20, Training Loss: 1.3829168865838684\n",
      "Epoch 10/20, Training Loss: 1.3788068379503655\n",
      "Epoch 11/20, Training Loss: 1.3765765258842546\n",
      "Epoch 12/20, Training Loss: 1.373994446123939\n",
      "Epoch 13/20, Training Loss: 1.3747096779834793\n",
      "Epoch 14/20, Training Loss: 1.3736125485297257\n",
      "Epoch 15/20, Training Loss: 1.372895422829203\n",
      "Epoch 16/20, Training Loss: 1.3750756927351806\n",
      "Epoch 17/20, Training Loss: 1.3765832916440734\n",
      "Epoch 18/20, Training Loss: 1.3774741507438293\n",
      "Epoch 19/20, Training Loss: 1.378143287917286\n",
      "Epoch 20/20, Training Loss: 1.3799517101261443\n",
      "Total execution time for training: 471.5668182373047 seconds\n",
      "Accuracy on test set: 56.41286562815197%\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Download and prepare the dataset\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "response = requests.get(url)\n",
    "text = response.text\n",
    "\n",
    "sequence_length = 20\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_int = {ch: i for i, ch in enumerate(chars)}\n",
    "int_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "encoded_text = [char_to_int[ch] for ch in text]\n",
    "\n",
    "sequences = []\n",
    "targets = []\n",
    "for i in range(0, len(encoded_text) - sequence_length):\n",
    "    seq = encoded_text[i:i+sequence_length]\n",
    "    target = encoded_text[i+sequence_length]\n",
    "    sequences.append(seq)\n",
    "    targets.append(target)\n",
    "\n",
    "sequences = torch.tensor(sequences, dtype=torch.long)\n",
    "targets = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, sequences, targets):\n",
    "        self.sequences = sequences\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.sequences[index], self.targets[index]\n",
    "\n",
    "dataset = CharDataset(sequences, targets)\n",
    "\n",
    "batch_size = 128\n",
    "train_size = int(len(dataset) * 0.8)\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "# Step 2: Define the GRU model\n",
    "class CharGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(CharGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        embedded = self.embedding(x)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        output = self.fc(output[:, -1, :])\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)\n",
    "\n",
    "# Step 3: Instantiate the model, loss function, and optimizer\n",
    "input_size = len(chars)\n",
    "hidden_size = 256\n",
    "output_size = len(chars)\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = CharGRU(input_size, hidden_size, output_size).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Step 4: Training the model\n",
    "epochs = 20\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        hidden = model.init_hidden(data.size(0))\n",
    "        output, hidden = model(data, hidden)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Total execution time for training: {execution_time} seconds\")\n",
    "\n",
    "# Step 5: Evaluate the model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        hidden = model.init_hidden(data.size(0))\n",
    "        output, hidden = model(data, hidden)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "print(f\"Accuracy on test set: {100 * correct / total}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qsqn8dIf8FCZ"
   },
   "source": [
    "GRU for Sequences of 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "iUSNsLXm8G0o"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Training Loss: 1.6845021229696835\n",
      "Epoch 2/20, Training Loss: 1.4907634529107203\n",
      "Epoch 3/20, Training Loss: 1.4462939730438773\n",
      "Epoch 4/20, Training Loss: 1.4209519840405032\n",
      "Epoch 5/20, Training Loss: 1.4030264049845391\n",
      "Epoch 6/20, Training Loss: 1.3909033857760453\n",
      "Epoch 7/20, Training Loss: 1.3831645669314072\n",
      "Epoch 8/20, Training Loss: 1.376318691006626\n",
      "Epoch 9/20, Training Loss: 1.3696787143430693\n",
      "Epoch 10/20, Training Loss: 1.3671412442138122\n",
      "Epoch 11/20, Training Loss: 1.363532286776391\n",
      "Epoch 12/20, Training Loss: 1.362036306695287\n",
      "Epoch 13/20, Training Loss: 1.3613632699315465\n",
      "Epoch 14/20, Training Loss: 1.3603778393341766\n",
      "Epoch 15/20, Training Loss: 1.3611623440931668\n",
      "Epoch 16/20, Training Loss: 1.3613169246699983\n",
      "Epoch 17/20, Training Loss: 1.362958763241426\n",
      "Epoch 18/20, Training Loss: 1.3644108500030283\n",
      "Epoch 19/20, Training Loss: 1.368361405566617\n",
      "Epoch 20/20, Training Loss: 1.3688058824424285\n",
      "Total execution time for training: 577.7683596611023 seconds\n",
      "Accuracy on test set: 56.64423753659116%\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Download and prepare the dataset\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "response = requests.get(url)\n",
    "text = response.text\n",
    "\n",
    "sequence_length = 30\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_int = {ch: i for i, ch in enumerate(chars)}\n",
    "int_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "encoded_text = [char_to_int[ch] for ch in text]\n",
    "\n",
    "sequences = []\n",
    "targets = []\n",
    "for i in range(0, len(encoded_text) - sequence_length):\n",
    "    seq = encoded_text[i:i+sequence_length]\n",
    "    target = encoded_text[i+sequence_length]\n",
    "    sequences.append(seq)\n",
    "    targets.append(target)\n",
    "\n",
    "sequences = torch.tensor(sequences, dtype=torch.long)\n",
    "targets = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, sequences, targets):\n",
    "        self.sequences = sequences\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.sequences[index], self.targets[index]\n",
    "\n",
    "dataset = CharDataset(sequences, targets)\n",
    "\n",
    "batch_size = 128\n",
    "train_size = int(len(dataset) * 0.8)\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "# Step 2: Define the GRU model\n",
    "class CharGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(CharGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        embedded = self.embedding(x)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        output = self.fc(output[:, -1, :])\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)\n",
    "\n",
    "# Step 3: Instantiate the model, loss function, and optimizer\n",
    "input_size = len(chars)\n",
    "hidden_size = 256\n",
    "output_size = len(chars)\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = CharGRU(input_size, hidden_size, output_size).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Step 4: Training the model\n",
    "epochs = 20\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        hidden = model.init_hidden(data.size(0))\n",
    "        output, hidden = model(data, hidden)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Total execution time for training: {execution_time} seconds\")\n",
    "\n",
    "# Step 5: Evaluate the model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        hidden = model.init_hidden(data.size(0))\n",
    "        output, hidden = model(data, hidden)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "print(f\"Accuracy on test set: {100 * correct / total}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ej4gWnQX8SDq"
   },
   "source": [
    "GRU for Sequences of 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "PKKMyHpi8Tpf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Training Loss: 1.6898421757430877\n",
      "Epoch 2/20, Training Loss: 1.488524606555867\n",
      "Epoch 3/20, Training Loss: 1.4408102502543791\n",
      "Epoch 4/20, Training Loss: 1.4152128369731927\n",
      "Epoch 5/20, Training Loss: 1.3974233954661786\n",
      "Epoch 6/20, Training Loss: 1.3845649699093605\n",
      "Epoch 7/20, Training Loss: 1.3758783903496112\n",
      "Epoch 8/20, Training Loss: 1.3680664905857414\n",
      "Epoch 9/20, Training Loss: 1.3630427946029957\n",
      "Epoch 10/20, Training Loss: 1.3593863217359703\n",
      "Epoch 11/20, Training Loss: 1.3564262528139726\n",
      "Epoch 12/20, Training Loss: 1.3537577885822831\n",
      "Epoch 13/20, Training Loss: 1.3524633977765794\n",
      "Epoch 14/20, Training Loss: 1.350524461324101\n",
      "Epoch 15/20, Training Loss: 1.3530915290514856\n",
      "Epoch 16/20, Training Loss: 1.353417439476829\n",
      "Epoch 17/20, Training Loss: 1.3531882635251877\n",
      "Epoch 18/20, Training Loss: 1.3544715554689235\n",
      "Epoch 19/20, Training Loss: 1.3554600750146508\n",
      "Epoch 20/20, Training Loss: 1.357957244594852\n",
      "Total execution time for training: 783.5927467346191 seconds\n",
      "Accuracy on test set: 56.98864476910732%\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Download and prepare the dataset\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "response = requests.get(url)\n",
    "text = response.text\n",
    "\n",
    "sequence_length = 50\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_int = {ch: i for i, ch in enumerate(chars)}\n",
    "int_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "encoded_text = [char_to_int[ch] for ch in text]\n",
    "\n",
    "sequences = []\n",
    "targets = []\n",
    "for i in range(0, len(encoded_text) - sequence_length):\n",
    "    seq = encoded_text[i:i+sequence_length]\n",
    "    target = encoded_text[i+sequence_length]\n",
    "    sequences.append(seq)\n",
    "    targets.append(target)\n",
    "\n",
    "sequences = torch.tensor(sequences, dtype=torch.long)\n",
    "targets = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, sequences, targets):\n",
    "        self.sequences = sequences\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.sequences[index], self.targets[index]\n",
    "\n",
    "dataset = CharDataset(sequences, targets)\n",
    "\n",
    "batch_size = 128\n",
    "train_size = int(len(dataset) * 0.8)\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "# Step 2: Define the GRU model\n",
    "class CharGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(CharGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        embedded = self.embedding(x)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        output = self.fc(output[:, -1, :])\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(1, batch_size, self.hidden_size, device=device))\n",
    "        \n",
    "# Step 3: Instantiate the model, loss function, and optimizer\n",
    "input_size = len(chars)\n",
    "hidden_size = 256\n",
    "output_size = len(chars)\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = CharGRU(input_size, hidden_size, output_size).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Step 4: Training the model\n",
    "epochs = 20\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        hidden = model.init_hidden(data.size(0))\n",
    "        output, hidden = model(data, hidden)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Total execution time for training: {execution_time} seconds\")\n",
    "\n",
    "# Step 5: Evaluate the model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        hidden = model.init_hidden(data.size(0))\n",
    "        output, hidden = model(data, hidden)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "print(f\"Accuracy on test set: {100 * correct / total}%\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
